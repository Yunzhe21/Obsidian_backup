
Let $\textbf{L}=D\boldsymbol{\Phi}(\textbf{x}_{0})$. It's sufficient to show that $\textbf{h}$ is a tangent vector if and only if $\textbf{L}(\textbf{h})=\textbf{0}$.
**On one hand,** Let $h\in T(\textbf{x}_{0})$, and let $\boldsymbol{\psi}$ be as in the definition of tangent vector. Then $\boldsymbol{\Phi}[\boldsymbol{\psi}(t)]=\textbf{0}$ for every $t\in (-\delta, \delta)$. Calculating the derivative of $\boldsymbol{\Phi}\circ\boldsymbol{\psi}$, we have $\textbf{0}=\textbf{L}[\boldsymbol{\psi}'(0)]=\textbf{L}(\textbf{h})$. 
**Conversely,** let $\textbf{L}(\textbf{h})=\textbf{0}$. We assume as before that the last $m$ columns of $\textbf{L}$ are linearly independent. Let $\textbf{f}$ be as in the proof of the implicit function theorem, and $U_{1}$ be the neighborhood of $\textbf{x}_0$ such that the restriction of $\textbf{f}$ to $U_{1}$ is regular. Thus there exists $\delta>0$ such that $\hat{\textbf{x}}+t\hat{\textbf{h}}\in R_{1}$ for every $t\in (-\delta,\delta)$. Let $\textbf{g}=(\textbf{f}|_{U_{1}})^{-1}$ and $\boldsymbol{\psi}(t)=\textbf{g}(\hat{\textbf{x}}_{0}+t\hat{\textbf{h}}, \textbf{0})$. Then $\boldsymbol{\psi}(t)\in M$ and $\boldsymbol{\psi}$ is of class $\mathscr{C}^{(1)}$
Now, we are left to show that $\boldsymbol{\psi}'(0)=\textbf{h}$. Let $\Lambda = D\textbf{f}(\textbf{x}_{0})$, then $\Lambda^{-1}$ is the differential of $\textbf{g}$ at the point $(\hat{\textbf{x}}_{0}, \textbf{0})=\textbf{f}(\textbf{x}_{0})$. Therefore $\boldsymbol{\psi}'(0)=\Lambda^{-1}(\hat{\textbf{h}},\textbf{0})$. By definition of $\textbf{f}$, $\Lambda^{i}(\textbf{h})=h^{i}$ for $i=1,\cdots, r$, and $\Lambda^{l+r}(\textbf{h})=d\boldsymbol{\psi}^{l}(\textbf{x}_{0})\cdot\textbf{h}$ for $l=1, \cdots, m$. Since $\textbf{h}$ is in the kernel of $D\boldsymbol{\Phi}(\textbf{x}_{0})$, $\Lambda^{l+r}(\textbf{h})=0$. Thus $\Lambda^{l+r}(\textbf{h})=0$. Thus $\Lambda(\textbf{h})=(\hat{h}, \textbf{0})$, and $\textbf{h}=\Lambda^{-1}(\hat{\textbf{h}}, \textbf{0})=\boldsymbol{\psi}'(0)$. ^5db12f

---

It's sufficient to consider only the constrained relative maximum case. Let $\textbf{h}$ be any tangent vector to $M$ at $\textbf{x}_{0}$. Let ${} \phi(t)=f[\boldsymbol{\psi}(t)] {}$, where $\boldsymbol{\psi}$ is the same as in the definition of tangent vector. Since $\boldsymbol{\psi}(t)\in M$ and $f|M$ has a relative maximum at $\textbf{x}_{0}$, $\phi$ has a relative maximum at $0$. Therefore $\phi'(0)=0$. By the chain rule, $\phi'(0)=\text{grad}f[\boldsymbol{\psi}(0)]\cdot\boldsymbol{\psi}'(0)=\text{grad}f(\textbf{x}_{0})\cdot\textbf{h}$, this show that $\text{grad}f(\textbf{x}_{0})$ is a normal vector to $M$ at $\textbf{x}_{0}$. However, $\text{grad}\boldsymbol{\Phi}^{1}(\textbf{x}_{0}), \cdots, \text{grad}\boldsymbol{\Phi}^{m}(\textbf{x}_{0})$ form a basis for the space of normal vectors to $M$ at $\textbf{x}_{0}$. Therefore, $\text{grad}f(\textbf{x}_{0})$ is a linear combination of the basis $a_{1}\text{grad}\boldsymbol{\Phi}^{1}(\textbf{x}_{0})+\cdots+a_{m}\text{grad}\boldsymbol{\Phi}^{m}(\textbf{x}_{0})$, let $\sigma_{i}=-a_{i}$, then $\text{grad}F(\textbf{x}_{0})=\textbf{0}$. ^da7b6b

---


